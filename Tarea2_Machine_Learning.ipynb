{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> INF393 - Máquinas de Aprendizaje  </h1>\n",
    "    <h2> Tarea 2 </h2>\n",
    "    <h3> Universidad Técnica Federico Santa Maria </h3>\n",
    "    \n",
    "</center>\n",
    "\n",
    "_Noviembre 2017_\n",
    "<p>Profesor: R. Ñanculef</p>\n",
    " <p>Ayudante: Francisco Mena</p>\n",
    " <p>Integrantes: \n",
    " <br>Alfredo Silva,\n",
    " 201373511-8</br>\n",
    " <br>Fernando Llorens, 201373528-2</br>\n",
    " \n",
    "\n",
    " \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Tipos de fronteras en Clasificación</h1>\n",
    "<p>Como se ha discutido en clases, los problemas de clasificación pueden ser representados de distintas\n",
    "maneras, en donde estas representaciones definen un espacio de entrada del dominio de los datos (RX). Los\n",
    "ejemplos dentro del espacio de entrada (manifold) pueden tener distintas formas, donde estas indicarán si es\n",
    "que estos ejemplos tendrán fronteras lineales o no. Con el propósito de analizar los distintos tipos de fronteras\n",
    "que definen los algoritmos de clasificación, se trabajará con un dataset sintético lo cual es ideal para analizar\n",
    "los diferentes tipos.</p>\n",
    "\n",
    "<p>Este dataset está en un espacio de 2 dimensiones y es conformado por dos conjuntos de datos, pudiendo\n",
    "ver que la frontera entre ellos claramente no es lineal. Uno de los conjuntos de datos es ovalado, generado\n",
    "a través de una distribución multivariada gaussiana, el otro conjunto de datos es una semi-luna, generado a\n",
    "través de funciones senos y cosenos. Se agrega ruido en los conjuntos para que no sea un problema trivial. El\n",
    "código que los genera es el siguiente:\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_samples=500\n",
    "mean = (0,-4)\n",
    "C = np.array([[0.3, 0.1], [0.1, 1.5]])\n",
    "datos1 = np.random.multivariate_normal(mean, C, n_samples)\n",
    "outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples))*3\n",
    "outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples))*3\n",
    "datos2 = np.vstack((outer_circ_x,outer_circ_y)).T\n",
    "from sklearn.utils import check_random_state\n",
    "generator = check_random_state(10)\n",
    "datos2 += generator.normal(scale=0.3, size=datos2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(a) Construya el conjunto de datos (dataset) común con los dos conjuntos generados. Luego se realiza un\n",
    "shift desde el conjunto 2 al 1, esto se puede ver en la imagen anterior, donde el conjunto de color\n",
    "naranjo (media luna) tiene puntos azules a la derecha pertenecientes al otro conjunto, esto es con\n",
    "el mismo propósito de trabajar con un dataset no ideal. Determine cuántos registros contiene cada\n",
    "conjunto y visualícelos</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlRUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe518eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.concatenate((datos1, datos2), axis=0)\n",
    "n = 20 #ruido/noise\n",
    "y1 = np.zeros(datos1.shape[0]+n)\n",
    "y2 = np.ones(datos2.shape[0]-n)\n",
    "y = np.concatenate((y1,y2),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Para visualizar las fronteras de los distintos algoritmos clasificadores utilice el siguiente código:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_border(model,x,y,title=\"\"):\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    4 plt.scatter(x[:,0], x[:,1], s=50, c=y, cmap=plt.cm.winter)\n",
    "    h = .02 # step size in the mesh\n",
    "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(b) Entrene el clasificador Linear Discriminant Analysis (LDA) y visualice la frontera de decisión que\n",
    "define este algoritmo. Analice cualitativamente lo que observa.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model = LDA()\n",
    "model.fit(X,y)\n",
    "visualize_border(model,X,y,\"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(c) Entrene el clasificador Quadratic Discriminant Analysis (QDA) y visualice la frontera de decisión que\n",
    "define este algoritmo. Analice cualitativamente lo que observa y compare con LDA, en qué difieren y\n",
    "en qué se asemejan ¿Qué distribución de probabilidad asumen cada uno?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "model = QDA()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(d) Compare cuantitativamente los clasificadores LDA Y QDA en este dataset sintético mediante la métrica\n",
    "de error de clasificación.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Miss Classification Loss: %f\"%(1-accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Para lo que sigue de la actividad se trabajará con una widget interactiva para sintonizar los parámetros\n",
    "de regularización de los distintos algoritmos. Por ello cada vez que se modifique el parámetro en la ventana\n",
    "se entrenará el modelo y se visualizará la frontera en la misma imagen.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "def visualize_border_interactive(param):\n",
    "    model = train_model(param)\n",
    "    visualize_border(model,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(e) Construya una función que entrene/ajuste un modelo de Regresión Logística Regularizado (utilizando\n",
    "como penalizador la norma l2), experimente con distintos valores del parámetro de regularización\n",
    "mediante el gráfico interactivo. Explique el significado y efecto esperado de este parámetro. Analice\n",
    "cualitativamente lo observado.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "def train_model(param):\n",
    "    model=LR() #define your model\n",
    "    model.set_params(C=param,penalty='l2')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "p_min = #define your range\n",
    "p_max = #define your range\n",
    "interactive(visualize_border_interactive,param=(p_min,p_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(f) Construya una función que entrene/ajuste una Máquina de Vectores de Soporte (SVM) Lineal. Mediante\n",
    "la imagen interactiva explore diferentes valores del parámetro de regularización C. Discuta el significado\n",
    "y efecto esperado de este parámetro. Analice cualitativamente lo observado.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='linear')\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "#use interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(g) Construya una función que entrene/ajuste una Máquina de Vectores de Soporte (SVM) no Lineal.\n",
    "Mediante la imagen interactiva explore diferentes valores del parámetro de regularización C y con\n",
    "diferentes kernels. Discuta el significado y efecto esperado de este parámetro. Analice cualitativamente\n",
    "lo observado.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#edit the train_model function\n",
    "model.set_params(C=param,kernel='rbf') #try poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(h) Construya un Arbol de Decisión de múltiples niveles para la clasificación del problema. Puede utilizar\n",
    "el criterio y la función de partición que prefiera. Mediante la imagen interactiva explore diferentes\n",
    "valores del parámetro de máxima profunidad del árbol. Discuta el significado y efecto esperado de este\n",
    "parámetro. Analice cualitativamente lo observado.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "model = Tree() #edit the train_model function\n",
    "model.set_params(max_depth=param,criterion='gini',splitter='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(i) Construya un algoritmo k-NN para la clasificación del problema. Mediante la imagen interactiva explore\n",
    "diferentes valores del parámetro k. Discuta el significado y efecto esperado de este parámetro. Analice\n",
    "cualitativamente lo observado.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.set_params(n_neighbors=param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Análisis de audios como datos brutos</h1>\n",
    "<p>Distintos tipos de datos han sido tratados en el área de Machine Learning, donde el análisis de estos y\n",
    "el manejo para poder dejarlos en una representación que se pueda entregar como entrada al algoritmo es\n",
    "crucial. El manejo sobre los datos brutos se denomina pre-procesamiento y existen distintos dependiento del\n",
    "tipo de datos y los distintos diminios de problemas, tales como imágenes, audios, texto.\n",
    "En esta actividad se trabajará con datos de audios los cuales son directamente extraídos desde datos fuentes\n",
    ".wav, lo que corresponde a una señal de sonido en diferentes tiempos.\n",
    "</p>\n",
    "<p>El dataset se denomina Heartbeat Sounds y es presentado en la plataforma Kaggle a través del siguiente\n",
    "link. Este dataset consta de grabaciones de sonidos de latidos cardíacos normales y anormales, con distintas\n",
    "categorías para los latidos anormales.\n",
    "Para la tarea se trabajará con el dataset A presente en la data, el cual corresponde a datos generados desde\n",
    "la vía pública mediante la aplicación de Iphone iStethoscope Pro. El objetivo será el de clasificar cada sonido\n",
    "como latido cardíaco normal o una de las las subcategorías de anormal (Murmur, Extra Heart Sound, Artifact),\n",
    "por lo que se trata de un problema de clasificación múltiple con 4 clases. Las distintas clasificaciones\n",
    "para los sonidos son explicadas en el sitio de Kaggle.\n",
    "</p>\n",
    "<p>Para leer y trabajar los archivos de extensión .wav se utilizará el siguiente código:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "def clean_filename(fname, string):\n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':\n",
    "    file_name = string + file_name\n",
    "    return file_name\n",
    "SAMPLE_RATE = 44100\n",
    "def load_wav_file(name, path):\n",
    "    s, b = wavfile.read(path + name)\n",
    "    assert s == SAMPLE_RATE\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(a) Construya un dataframe con los datos a analizar. Describa el dataset y determine cuántos registros hay\n",
    "por clase</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./heartbeat-sounds/set_a.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(b) Lea los archivos .wav y transformelos en secuencias de tiempo. Realice un padding de ceros al final de\n",
    "cada secuencia para que todas queden representadas con la misma cantidad de elementos, explique la\n",
    "importancia de realizar este paso.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padd_zeros(array,length):\n",
    "    aux = np.zeros(length)\n",
    "    aux[:array.shape[0]] = array\n",
    "    return aux\n",
    "new_df =pd.DataFrame({'file_name' : df['fname'].apply(clean_filename,string='Aunlabelled')})\n",
    "new_df['time_series'] = new_df['file_name'].apply(load_wav_file, path='path/to/set_a/')\n",
    "new_df['len_series'] = new_df['time_series'].apply(len)\n",
    "new_df['time_series']=new_df['time_series'].apply(padd_zeros,length=max(new_df['len_series']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(c) Manipule los datos y cambie las etiquetas de los audios por otras asignadas por un doctor experto [4],\n",
    "el cual afirma que estos cambios son requeridos. Vuelva a determinar cu´antos registros hay por clase.\n",
    "Nótese que ahora son 3 clases ¿Explique la problem´atica de tener etiquetas mal asignadas en los datos?\n",
    "¿Un solo dato puede afectar esto?</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
    "2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
    "1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
    "2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
    "0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "labels = ['artifact','normal/extrahls', 'murmur']\n",
    "new_df['target'] = [labels[i] for i in new_labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(d) Codifique las distintas clases a valores numéricos para que puedan ser trabajados por los algoritmos\n",
    "clasificadores.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df[\"target\"] = new_df[\"target\"].astype('category')\n",
    "cat_columns = new_df.select_dtypes(['category']).columns\n",
    "new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(e) Desordene los datos, evitando así el orden en el que vienen la gran mayoría de las etiquetas. Cree la\n",
    "matriz que conforma a los datos en sus dimensiones sin preprocesar, es decir, cada ejemplo es una\n",
    "secuencia de amplitudes en el tiempo. ¿Las dimensiones de ésta indica que puede generar problemas?\n",
    "¿De qué tipo?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = new_df.sample(frac=1,random_state=44)\n",
    "X = np.stack(new_df['time_series'].values, axis=0)\n",
    "y = new_df.target.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(f) Para pre procesar la secuencia en el tiempo realice una transformada de fourier discreta para pasar\n",
    "los datos desde el dominio de tiempos al dominio de frecuencias presentes en la señal de sonido.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fourier = np.abs(np.fft.fft(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(g) Para seguir con el pre-procesamiento realice un muestreo representativo de los datos a través de una\n",
    "técnica de muestreo especializada en secuencias ¿En qué beneficia este paso? ¿Cómo podría determinar\n",
    "si el muestro es representativo?</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "X_resampled = []\n",
    "for i in range(X_fourier.shape[0]):\n",
    "    sequence = X_fourier[i,:].copy()\n",
    "    resampled_sequence = signal.resample(sequence, 100000)\n",
    "    X_resampled.append(resampled_sequence)\n",
    "X_resampled = np.array(X_resampled)\n",
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(h) Genere un conjunto de pruebas mediante la técnica hold-out validation para verificar la calidad de los\n",
    "clasificadores. ¿Cuántas clases tiene y de qué tamaño queda cada conjunto?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(i) Realice un proceso de estandarizar los datos para ser trabajados adecuadamente. Recuerde que solo se\n",
    "debe ajustar (calcular media y desviación estándar) con el conjunto de entrenamiento.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(j) Realice una reducción de dimensionalidad a través de la técnica PCA, para representar los datos en\n",
    "d = 2 dimensiones. Recuerde que solo se debe ajustar (encontrar las componentes principales) con el\n",
    "conjunto de entrenamiento. Visualice apropiadamente la proyección en 2 dimensiones.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "d=2\n",
    "pca_model = PCA(n_components=d)\n",
    "pca_model.fit(X_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(k) Entrene un modelo de Regresión Logística variando el parámetro de regularizacion C construyendo un\n",
    "gráfico resumen del error en función de este hiper-parámetro. Además entrene una Máquina de Soporte\n",
    "Vectorial (SVM) con kernel lineal, variando el hiper-parámetro de regularizacion C en el mismo rango\n",
    "que para la Regresión Logística, construyendo el mismo gráfico resumen. Compare.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = [0.0001,0.01,0.1,1,10,100,1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(l) Entrene un Arbol de Decisión, con la configuración que estime conveniente, variando el hiper-parámetro\n",
    "regularizador max depth, construyendo un gráfico resumen del error en función de este parámetro.\n",
    "Compare con los modelos anteriores.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Depths = range(1,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(m) Experimente con diferentes dimensiones d para la proyección de PCA con el propósito de obtener un\n",
    "modelo con menor error. Construya una tabla o gráfico resumen.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aqui va codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(n) Realice otra reducción de dimensionalidad ahora a través de la técnica LDA, para representar los datos\n",
    "en d = 2 dimensiones. Recuerde que sólo se debe ajustar con el conjunto de entrenamiento, si se muestra\n",
    "un warning explique el por qué. Visualice apropiadamente la proyección en 2 dimensiones.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model_lda = LDA(n_components=2)\n",
    "model_lda.fit(X_train,y_train)\n",
    "X_pca_train = model_lda.transform(X_train)\n",
    "X_pca_test = model_lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(o) Con el propósito de encontrar el mejor modelo vuelva a realizar el item h) con el i) en el nuevo espacio\n",
    "generado por la representación según las d dimensiones de la proyección LDA. Esta nueva representación\n",
    "¿mejora o empeora el desempeño? Explique.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(p) Intente mejorar el desempeño de los algoritmos ya entrenados. Diseñe ahora sus propias cracterísticas\n",
    "(feature crafting) a partir de los datos brutos (secuencia de amplitudes), puede inspirarse en otros\n",
    "trabajos si desea.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Análisis de emociones en tweets</h1>\n",
    "<p>El análisis de emociones o sentimientos se refiere al proceso de extraer información acerca de la actitud\n",
    "que una persona (o grupo de ellas) manifiesta, en un determinado medio o formato digital, con respecto a un\n",
    "tópico o contexto de comunicación. Uno de los casos más estudiados corresponde a determinar la polaridad\n",
    "de un trozo de texto, es decir, clasificar una determinada evaluación escrita (review), en que una persona\n",
    "manifiesta una opinión, como positiva, negativa o neutral. Esto también ha sido extendido a otros medios,\n",
    "como lo es analizar la polaridad de textos en redes sociales.\n",
    "La conocida red social Twitter tiene una gran cantidad de usuarios, por lo que la información se genera a\n",
    "cada segundo, donde el análisis de texto se ha aplicado fuertemente a estos medios sociales. La dificultad de\n",
    "este problema radica en el carácter altamente ambiguo e informal del lenguaje que utilizan naturalmente las\n",
    "personas así como el manejo de negaciones, sarcasmo y abreviaciones en una frase.\n",
    "</p>\n",
    "<p>\n",
    "Para esta actividad se trabajará con un datasets de tweets ofrecidos por CrowdFlower. Cada tweet está\n",
    "asociado a una emoción en particular, donde el conjunto de emociones se trabajarán como mutuamente\n",
    "excluyentes, siendo un problema de múltiples clases.\n",
    "</p>\n",
    "<p>\n",
    "Para aumentar la eficacia de las características extraídas es conveniente ejecutar algunas técnicas de preprocesamiento\n",
    "básicas como: pasar todo el texto a minúsculas (lower-casing), eliminar signos de puntuación\n",
    "y eliminar palabras sin significado como artículos, pronombres y preposiciones (stop word removal). Otra\n",
    "técnica que suele ser útil para obtener buenas características (features) es la lematización, es decir la\n",
    "reducción de todas las palabras a su tronco léxico base. Una técnica similar y más utilizada en la práctica es\n",
    "el stemming. Varias de éstas están implementadas en la libreria nltk para python.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(a) Construya un dataframe con los datos a analizar. Determine cuántas clases existen, cuántos registros\n",
    "por clase y describa el dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 4 columns):\n",
      "tweet_id     40000 non-null int64\n",
      "sentiment    40000 non-null object\n",
      "author       40000 non-null object\n",
      "content      40000 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.2+ MB\n",
      "@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./text_emotion.csv')\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "print(df['content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(b) Construya un conjunto de entrenamiento y otro de pruebas, a través de una máscara aleatoria, para\n",
    "verificar los resultados de los algoritmos.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de entrenamiento: 32061\n",
      "Tamaño de prueba: 7939\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "print(\"Tamaño de entrenamiento: {}\".format(len(df_train)))\n",
    "print(\"Tamaño de prueba: {}\".format(len(df_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(c) Implemente y explique un pre-procesamiento para los tweets para dejarlos en un formato estándarizado\n",
    "en el cual se podrán trabajar.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se asume que todos los tweets estan en ingles\n",
    "#Se ocupa Snowball, por que en inglés se comporta mejor que Porter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "def preprocesamiento(texto):\n",
    "    #En caso de que aparezcan simbolos raros, agregar a la lista\n",
    "    stoplist = stopwords.words('english')\n",
    "    listastop=[\"'\",\".\",\"#\",\"<\",\">\",\"}\",\"{\",\"]\",\"[\",\"´\",\"?\",\"¡\",\"!\",\"¿\",\"/\",\"*\",\n",
    "                '\"',\"-\",\"--\",\";\",\":\",\"::\",\"=\",\")\",\"(\",\"&\",\"|\",\"'ve\",\"'t\",\"'s\",\n",
    "               \"'ll\",\"n't\",\">.<\",\"<3\",\"._.\",\"~\",\"<br\",\"</\",\"/>\",\"<<\",\">>\",\"~~\",\n",
    "               \"...\",\",\",\"d\",\"na\",\"m\",\"''\",\"´´\",\"!!!!!\",'\"(','\",','.;','[*','tm',\n",
    "               '\").',\"l\",'\"&',').*','://','www','http','\"...',\"=[\"]\n",
    "    nuevostoplist= stoplist+listastop\n",
    "    tokens = nltk.wordpunct_tokenize(sentencia)\n",
    "    filtered = [token.decode('utf-8').lower() for token in tokens if token not in nuevostoplist]\n",
    "    \n",
    "    stemmed = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for word in filtered:\n",
    "        stemmed.append(stemmer.stem(word))\n",
    "    return filtered, stemmed\n",
    "    \n",
    "    #stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(d) Haga una reducción binaria al problema, para trabajarlo como un problema de clasificación de dos clases.\n",
    "Para esto, agrupe las distintas emociones como positivas y negativas (defina un criterio), se recomienda\n",
    "codificar las clases como +1 y −1 respectivamente. Recuerde tener presente que el desbalanceo de los\n",
    "datos puede afectar considerablemente al modelo.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(e) Para construir un clasificador que determine automáticamente la polaridad de un trozo de texto, será\n",
    "necesario representar los tweets {ti}i=1->n disponibles como vectores de características (features). El tipo\n",
    "de características más utilizado consiste en contar cuántas veces aparecen ciertos términos/palabras en\n",
    "el texto. Para esto, es necesario un vocabulario que, por lo general, se construye mediante la unión de\n",
    "todas las palabras que se observen en los tweets.\n",
    "</p>\n",
    "<p>\n",
    "Se recomienda utilizar las librerías ofrecidas por sklearn de feature extraction in text (CountVectorizer\n",
    "y TfidfVectorizer ). Recuerde realizar el ajuste (fit) únicamente con el conjunto de entrenamiento,\n",
    "para luego transformar el conjunto de pruebas (con el método transform).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(f) Entrene y compare al menos 5 de los diferentes clasificadores vistos en clases para clasificación binaria\n",
    "(por ejemplo: Navie Bayes, Multinomial Naive Bayes, LDA, QDA, Regresión logística, SVM y Arboles\n",
    "de decisión) sobre el conjunto de entrenamiento verificando su desempeño sobre ambos conjuntos\n",
    "(entrenamiento y de pruebas), construyendo un gráfico resumen del error de éstos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(g) Utilice y explique las métricas que calcula la función classification report de la librería sklearn. En base\n",
    "a las distintas métricas calculadas ¿Cuáles clasificadores son los que mejor se comportan?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def score_the_model(model,x,y,xt,yt):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy: %f\"%(acc_tr)\n",
    "    print \"Test Accuracy: %f\"%(acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(y_test, model.predict(X_test), target_names=['+','-']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(h) [Opcional] Visualice las predicciones de algún modelo generativo (probabilístico) definido anteriormente,\n",
    "tomando un subconjunto aleatorio de tweets de pruebas y explorando las probabilidades que asigna el\n",
    "clasificador a cada clase.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict_proba(X_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(df_test.content[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(i) Ahora deberá extender el problema a las múltiples clases que tiene presente (las distintas emociones),\n",
    "es decir, su trabajo será el de predecir una de las distintas emociones de cada tweet. Para esto utilice el\n",
    "mismo pre-procesamiento realizado en el punto c) y las características generadas mediante las técnicas\n",
    "en el punto e). Recuerde que tendrá que codificar las distintas clases como valores numéricos enteros.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(j) Utilice los clasificadores que son extendidos por defecto a múltiples clases para detectar emociones en\n",
    "cada tweet, muestre sus desempeños a través del error de pruebas en un gráfico resumen.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(k) Utilice clasificadores binarios que pueden ser extendidos a través de otras técnicas, tal como One vs\n",
    "One y One vs All/Rest.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "#example\n",
    "classif = OneVsRestClassifier(model)\n",
    "classif.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(l) Para el caso de la Regresión Logística compare sus dos métodos para ser extendidos a múltiples clases.\n",
    "Uno a través de One vs Rest y otro definiendo que la variable a predecir se distribuye Multinomial.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression(multi_class='ovr')\n",
    "LogisticRegression(multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(m) Compare los resultados entre los clasificadores extendidos por defecto y los binarios que son extendidos\n",
    "mediante otras técnicas, construya una tabla o gráfico resumen. Los clasificadores que mejor se\n",
    "comportan en el caso binario ¿Siguen teniendo ese desempeño en múltiples clases?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
